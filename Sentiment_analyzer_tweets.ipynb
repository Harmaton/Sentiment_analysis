{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Pyolite",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "pd.options.display.max_colwidth = 100\ntrain_data = pd.read_csv(\"../input/train.csv\", encoding='ISO-8859-1')",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "rand_indexs = np.random.randint(1,len(train_data),50).tolist()\ntrain_data[\"SentimentText\"][rand_indexs]",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import re\ntweets_text = train_data.SentimentText.str.cat()\nemos = set(re.findall(r\" ([xX:;][-']?.) \",tweets_text))\nemos_count = []\nfor emo in emos:\nemos_count.append((tweets_text.count(emo), emo))\nsorted(emos_count,reverse=True)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "HAPPY_EMO = r\" ([xX;:]-?[dD)]|:-?[\\)]|[;:][pP]) \"\nSAD_EMO = r\" (:'?[/|\\(]) \"\nprint(\"Happy emoticons:\", set(re.findall(HAPPY_EMO, tweets_text)))\nprint(\"Sad emoticons:\", set(re.findall(SAD_EMO, tweets_text)))",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# #### Most used words\n# What we are going to do next is to define a function that will show us top words, so we may fix things before running our learning algorithm. This function takes as input a text and output words sorted according to their frequency, starting with the most used word.\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import nltk\nfrom nltk.tokenize import word_tokenize",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "nltk.download('punkt')\ndef most_used_words(text):\ntokens = word_tokenize(text)\nfrequency_dist = nltk.FreqDist(tokens)\nprint(\"There is %d different words\" % len(set(tokens)))\nreturn sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "most_used_words(train_data.SentimentText.str.cat())[:100]",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# #### Stop words\n\n# What we can see is that stop words are the most used, but in fact they don't help us determine if a tweet is happy/sad, however, they are consuming memory and they are making the learning process slower, so we really need to get rid of them.\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from nltk.corpus import stopwords",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "nltk.download(\"stopwords\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "mw = most_used_words(train_data.SentimentText.str.cat())\nmost_words = []\nfor w in mw:\nif len(most_words) == 1000:\nbreak\nif w in stopwords.words(\"english\"):\n    continue\nelse:\nmost_words.append(w)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Stemming",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "nltk.download('wordnet')\ndef stem_tokenize(text):\nstemmer = SnowballStemmer(\"english\")\nstemmer = WordNetLemmatizer()\nreturn [stemmer.lemmatize(token) for token in word_tokenize(text)]\n\ndef lemmatize_tokenize(text):\nlemmatizer = WordNetLemmatizer()\nreturn [lemmatizer.lemmatize(token) for token in word_tokenize(text)]",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import TfidfVectorizer",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.pipeline import Pipeline",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# We need to do some preprocessing of the tweets.\n# We will delete useless strings (like @, # ...)\n# because we think that they will not help\n# in determining if the person is Happy/Sad",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "class TextPreProc(BaseEstimator,TransformerMixin):\ndef __init__(self, use_mention=False):\n    self.use_mention = use_mention\n  \n  def fit(self, X, y=None):\n        return self\n  \n  def transform(self, X, y=None):\n        if self.use_mention:\n            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \" @tags \")\n        else:\n            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \"\")\n    # Keeping only the word after the #\n            X = X.str.replace(\"#\", \"\")\n            X = X.str.replace(r\"[-\\.\\n]\", \"\")\n# Removing HTML garbage\n            X = X.str.replace(r\"&\\w+;\", \"\")\n# Removing links\n            X = X.str.replace(r\"https?://\\S*\", \"\")\n# replace repeated letters with only two occurences\n# heeeelllloooo => heelloo\n            X = X.str.replace(r\"(.)\\1+\", r\"\\1\\1\")\n# mark emoticons as happy or sad\n            X = X.str.replace(HAPPY_EMO, \" happyemoticons \")\n            X = X.str.replace(SAD_EMO, \" sademoticons \")\n            X = X.str.lower()\n            return X",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# This is the pipeline that will transform our tweets to something eatable.\n# You can see that we are using our previously defined stemmer, it will\n# take care of the stemming process.\n# For stop words, we let the inverse document frequency do the job",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "sentiments = train_data['Sentiment']\ntweets = train_data['SentimentText']",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\nvectorizer = TfidfVectorizer(tokenizer=lemmatize_tokenize, ngram_range=(1,2))\npipeline = Pipeline([\n('text_pre_processing', TextPreProc(use_mention=True)),\n('vectorizer', vectorizer),\n])",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Let's split our data into learning set and testing set\n# This process is done to test the efficency of our model at the end.\n# You shouldn't look at the test data only after choosing the final model",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "learn_data, test_data, sentiments_learning, sentiments_test = train_test_split(tweets, sentiments, test_size=0.3)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# This will tranform our learning data from simple text to vector\n# by going through the preprocessing tranformer.\nlearning_data = pipeline.fit_transform(learn_data)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Select a model",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\n\nlr = LogisticRegression()\nbnb = BernoulliNB()\nmnb = MultinomialNB()\n\nmodels = {\n'logitic regression': lr,\n'bernoulliNB': bnb,\n'multinomialNB': mnb,\n}\n\nfor model in models.keys():\nscores = cross_val_score(models[model], learning_data, sentiments_learning, scoring=\"f1\", cv=10)\nprint(\"===\", model, \"===\")\nprint(\"scores = \", scores)\nprint(\"mean = \", scores.mean())\nprint(\"variance = \", scores.var())\nmodels[model].fit(learning_data, sentiments_learning)\nprint(\"score on the learning data (accuracy) = \", accuracy_score(models[model].predict(learning_data), sentiments_learning))\nprint(\"\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Fine tune the model\n\n# I'm going to use the GridSearchCV to choose the best parameters to use.\n#\n# What the GridSearchCV does is trying different set of parameters, and for each one, it runs a cross validation and estimate the score.",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import GridSearchCV\n\ngrid_search_pipeline = Pipeline([\n('text_pre_processing', TextPreProc()),\n('vectorizer', TfidfVectorizer()),\n('model', MultinomialNB()),\n])\n\nparams = [\n{\n'text_pre_processing__use_mention': [True, False],\n'vectorizer__max_features': [1000, 2000, 5000, 10000, 20000, None],\n'vectorizer__ngram_range': [(1,1), (1,2)],\n},\n]\ngrid_search = GridSearchCV(grid_search_pipeline, params, cv=5, scoring='f1')\ngrid_search.fit(learn_data, sentiments_learning)\nprint(grid_search.best_params_)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "mnb.fit(learning_data, sentiments_learning)\ntesting_data = pipeline.transform(test_data)\nmnb.score(testing_data, sentiments_test)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Predecting on the test.csv\nsub_data = pd.read_csv(\"../input/test.csv\", encoding='ISO-8859-1')\nsub_learning = pipeline.transform(sub_data.SentimentText)\nsub = pd.DataFrame(sub_data.ItemID, columns=(\"ItemID\", \"Sentiment\"))\nsub[\"Sentiment\"] = mnb.predict(sub_learning)\nprint(sub)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Just run it\nmodel = MultinomialNB()\nmodel.fit(learning_data, sentiments_learning)\ntweet = pd.Series([input(),])\ntweet = pipeline.transform(tweet)\nproba = model.predict_proba(tweet)[0]\nprint(\"The probability that this tweet is sad is:\", proba[0])\nprint(\"The probability that this tweet is happy is:\", proba[1])",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}